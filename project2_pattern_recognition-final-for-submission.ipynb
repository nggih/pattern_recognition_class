{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#dataset_name = 'data_set' + letter + '.dat'\"\n",
    "data = np.loadtxt( 'data_setA.dat' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i be the row index which increased from bottom to top of each character.\n",
    "# j be the column index which increased from left to right of the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b0e96d2978>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAD8CAYAAAAsX4y/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b0e5943eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pattern = data[0:25]\n",
    "pattern = np.flipud(pattern)\n",
    "# Matplotlib imshow to mapping the 0 and 1 into black and white pattern in the image, respectively.\n",
    "plt.imshow(pattern, cmap='gray', interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_title(printing=False):\n",
    "    characters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "    indexes = []\n",
    "    for char in characters:\n",
    "        for i in range(1, 10+1):\n",
    "            index = char + str(i)\n",
    "            indexes.append(index)\n",
    "        if printing==True:\n",
    "            indexes.append('mean(' + char +')')\n",
    "            indexes.append('var(' + char + ')')\n",
    "\n",
    "    #print(indexes)\n",
    "    row_names = np.array(indexes)\n",
    "    row_names = pd.DataFrame(row_names)\n",
    "    return characters, list(row_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "I = [i for i in range(1,25+1)] # An array containing indexes in one of column\n",
    "J = [i for i in range(1,35+1)] # An array containing indexes in one of row\n",
    "#print(col, row)\n",
    "\n",
    "def centroid(init, last, data):\n",
    "    pattern = data[init:last]\n",
    "    pattern = np.flipud(pattern)\n",
    "    sum_sum_Xij = np.sum(pattern)\n",
    "    sum_sum_Xij_times_i = sum(np.dot(pattern.T, I))\n",
    "    sum_sum_Xij_times_j = sum(np.dot(pattern, J))\n",
    "    \n",
    "    imean = sum_sum_Xij_times_i/sum_sum_Xij\n",
    "    jmean = sum_sum_Xij_times_j/sum_sum_Xij\n",
    "    \n",
    "    return pattern, imean, jmean\n",
    "\n",
    "def mpq(pattern, imean, jmean, p, q):\n",
    "    # The (p,q)th Central Moment of a pattern X is:\n",
    "    i_part = (I-imean)**p\n",
    "    i_part = np.reshape(i_part, (25,1))\n",
    "    \n",
    "    j_part = (J-jmean)**q\n",
    "    j_part = np.reshape(j_part, (35, 1))\n",
    "    \n",
    "    step = pattern * i_part\n",
    "    Mpq = np.sum(np.dot(step, j_part))\n",
    "    #Mpq = step * j_part\n",
    "    return float(Mpq)\n",
    "'''\n",
    "def mpq(pattern, imean, jmean, p, q):\n",
    "    # The (p,q)th Central Moment of a pattern X is:\n",
    "    total = 0\n",
    "    for i in range(len(I)):\n",
    "        for j in range(len(J)):\n",
    "            total += (I[i] - imean)**p * (J[j] - jmean)**q * pattern[i][j]\n",
    "    \n",
    "    return total\n",
    "'''\n",
    "\n",
    "p = [0,0,1,2,0,1,2,3]\n",
    "q = [0,2,1,0,3,2,1,0]\n",
    "\n",
    "# Root mean square function.\n",
    "def rms(df):\n",
    "    df = np.array(df) **2\n",
    "    rms = np.sqrt(np.mean(df))\n",
    "    return rms\n",
    "\n",
    "def all_rms(p, q, data):\n",
    "    moments = [] # Moments of 100 samples\n",
    "    overall_rms = []\n",
    "    M = []\n",
    "    mean = []\n",
    "    var = []\n",
    "    \n",
    "    for index_p, index_q in zip(p,q):\n",
    "        column_names = \"M\" + str(index_p) + str(index_q)\n",
    "        M.append(column_names)\n",
    "        \n",
    "        central_moments = []\n",
    "\n",
    "        init = 0\n",
    "        last = 25\n",
    "        counter = 0\n",
    "    \n",
    "        while last <= len(data):\n",
    "            \n",
    "            pattern, imean, jmean = centroid(init,last, data)\n",
    "            Mpq = mpq(pattern, imean, jmean, index_p, index_q)\n",
    "            central_moments.append(Mpq)\n",
    "    \n",
    "            # Put mean and var of the normalized central moments here\n",
    "            \n",
    "            # Direct answer for the variance in the set of means.\n",
    "            #np.var already divides by N rather than N-1\n",
    "            \n",
    "            # Break point. It's time to stop.\n",
    "            init += 25\n",
    "            last += 25\n",
    "            \n",
    "        # 100 central_moments will be divided into 10\n",
    "        \n",
    "        RMS = rms(central_moments)\n",
    "        moments.append(central_moments/RMS)\n",
    "        \n",
    "        overall_rms.append(RMS)\n",
    "       \n",
    "        # Clearing\n",
    "        central_moments = []\n",
    "        \n",
    "    return M, pd.DataFrame(moments), overall_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_and_var(Mpq, central_moments):\n",
    "    k = 0\n",
    "    l = 10\n",
    "    all_means = []\n",
    "    all_var = []\n",
    "    central_moments.columns = Mpq\n",
    "    while l <= 100:\n",
    "        target = central_moments[k:l]\n",
    "        means = []\n",
    "        var = []\n",
    "        for i in Mpq:\n",
    "            element_mean = np.mean(target[i])\n",
    "            # Direct answer for the variance in the set of means.\n",
    "            element_var = np.var(target[i]) #np.var already divides by N rather than N-1\n",
    "            means.append(element_mean)\n",
    "            var.append(element_var)\n",
    "        all_means.append(means)\n",
    "        all_var.append(var)\n",
    "        means = []\n",
    "        var = []\n",
    "        k += 10\n",
    "        l += 10\n",
    "\n",
    "    #print(\"mean\", all_means)\n",
    "    #print(\"var\", all_var)\n",
    "\n",
    "    mean_df = np.array(all_means)\n",
    "    mean_df = pd.DataFrame(mean_df)\n",
    "\n",
    "    var_df = np.array(all_var)\n",
    "    var_df = pd.DataFrame(var_df)\n",
    "    \n",
    "    return mean_df, var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def indexing_nms(data):\n",
    "    Mpq, moments, overall_rms = all_rms(p,q,data)\n",
    "    normalized_cm = moments.transpose()\n",
    "\n",
    "    mean_df, var_df = mean_and_var(Mpq, normalized_cm)\n",
    "    mean_df.columns = Mpq\n",
    "    var_df.columns = Mpq\n",
    "    characters, indexes = index_title()\n",
    "\n",
    "    title_add = []\n",
    "    for i in range(len(mean_df)):\n",
    "        normalized_cm = pd.concat([normalized_cm, mean_df.loc[[i]]])\n",
    "        normalized_cm = pd.concat([normalized_cm, var_df.loc[[i]]])\n",
    "        title_add.append('mean('+ characters[i]+')')\n",
    "        title_add.append('var('+ characters[i]+')')\n",
    "\n",
    "    overall_rms = pd.DataFrame(overall_rms).T\n",
    "    overall_rms.columns = Mpq\n",
    "    normalized_cm = pd.concat([normalized_cm, overall_rms.loc[[0]]])\n",
    "\n",
    "    new_indexes = indexes + title_add + ['overall_rms']\n",
    "    \n",
    "    normalized_cm.index = new_indexes\n",
    "    print('Indexing finished, generate normalized central moments')\n",
    "    return normalized_cm, mean_df, var_df, overall_rms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov(X0):\n",
    "    X = X0 - X0.mean(axis=0)\n",
    "    N = X.shape[0]                # !!!# Divide by N\n",
    "    covariance = np.dot(X.T, X)/N\n",
    "    return covariance # !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ncm_to_report(csv_file, normalized_data, mean_df, var_df, overall_rms):\n",
    "    mean_df = mean_df\n",
    "    var_df = var_df\n",
    "    overall_rms = overall_rms\n",
    "    letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "    for i in range(0,10):\n",
    "        init=i * 10\n",
    "        last = init+10\n",
    "        \n",
    "        ncm = pd.DataFrame(normalized_data[init:last])\n",
    "        ncm.to_csv(csv_file, mode='a', float_format='%.3f')\n",
    "        \n",
    "        mean_target = mean_df.iloc[[i]]\n",
    "        mean_target.index = ['mean(' + letters[i] + ')']\n",
    "        mean_target.to_csv(csv_file, mode='a', header=None, float_format='%.3f')\n",
    "        \n",
    "        var_target = var_df.iloc[[i]]\n",
    "        var_target.index = ['var(' + letters[i] + ')']\n",
    "        var_target.to_csv(csv_file, mode='a', header=None, float_format='%.3f')\n",
    "    \n",
    "    overall_rms.index = ['Overall RMS']\n",
    "    overall_rms.to_csv(csv_file, mode='a')\n",
    "    \n",
    "    line_break = pd.DataFrame([\" \"])\n",
    "    line_break.to_csv(csv_file, mode='a', header=None, index=None)\n",
    "    \n",
    "    print(\"Ncm is written to %s\" %csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov_inv_to_report(csv_file, normalized_data, dataset_name):\n",
    "    from numpy.linalg import inv\n",
    "    letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "    line_break = pd.DataFrame([\" \"])\n",
    "    # To name the index as Inverse-Cov-Dataset-nth or Cov-Dataset-nth\n",
    "    def index_title(mode, dataset_name, letter):\n",
    "        title = ''\n",
    "        if mode == \"inverse\":\n",
    "            title = 'Inverse-Cov-Dataset-' + dataset_name + '-'+ letter\n",
    "        else:\n",
    "            title = 'Cov-Dataset-' + dataset_name + '-' + letter\n",
    "        the_title = [title] + [' ' for i in range(7)]\n",
    "        return the_title\n",
    "    \n",
    "    saved_cov = np.zeros(8) \n",
    "    container_cov = []\n",
    "    # Running through for the Covariance\n",
    "    for i in range(0,10):\n",
    "        init=i * 10\n",
    "        last = init+10\n",
    "        cov_letter = cov(normalized_data[init:last])\n",
    "        saved_cov = np.add(saved_cov,cov_letter)\n",
    "        cov_a = pd.DataFrame(cov_letter)\n",
    "        container_cov.append(cov_a)\n",
    "        cov_a.index = index_title(\"cov\", dataset_name, letters[i])\n",
    "        cov_a.to_csv(csv_file, mode='a', header=None, float_format='%.5f')\n",
    "   \n",
    "        line_break.to_csv(csv_file, mode='a', header=None, index=None)\n",
    "    \n",
    "    container_inv = []\n",
    "    # Running through for the Inverse of the Covariance\n",
    "    for i in range(0,10):\n",
    "        init=i * 10\n",
    "        last = init+10\n",
    "        inv_letter = inv(cov(normalized_data[init:last]))\n",
    "        inv_a = pd.DataFrame(inv_letter)\n",
    "        container_inv.append(inv_a)\n",
    "        inv_a.index = index_title(\"inverse\", dataset_name, letters[i])\n",
    "        inv_a.to_csv(csv_file, mode='a', header=None, float_format='%.5f')\n",
    "        line_break.to_csv(csv_file, mode='a', header=None, index=None)\n",
    "    \n",
    "    average_cov = saved_cov/10\n",
    "    average_cov_inversed = inv(average_cov)\n",
    "    \n",
    "    average_cov = pd.DataFrame(average_cov)\n",
    "    average_cov.index = ['Cov-Dataset-'+dataset_name] + [' ' for i in range(7)]\n",
    "    average_cov.to_csv(csv_file, mode='a', header=None, float_format='%.6f')\n",
    "    \n",
    "    line_break.to_csv(csv_file, mode='a', header=None, index=None)\n",
    "    \n",
    "    average_cov_inversed = pd.DataFrame(average_cov_inversed)\n",
    "    average_cov_inversed.index = ['Inverse-Cov-Dataset-'+dataset_name] + [' ' for i in range(7)]\n",
    "    average_cov_inversed.to_csv(csv_file, mode='a', header=None, float_format='%.6f')\n",
    "    \n",
    "    #print('Covariance and its Inverse are written to %s!' % csv_file)\n",
    "    return average_cov, average_cov_inversed, container_cov, container_inv\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_reports(adding_suffix):\n",
    "    list_datasets = ['A', 'B', 'C', 'D']\n",
    "\n",
    "    for i in range(len(list_datasets)):\n",
    "        letter = list_datasets[i]\n",
    "        label = 'data_set' + letter\n",
    "        dataset = label +'.dat'\n",
    "        data = np.loadtxt(dataset)\n",
    "        normalized_cm, mean_df, var_df, overall_rms = indexing_nms(data)\n",
    "\n",
    "        label = label + adding_suffix\n",
    "        ncm_to_report(label, normalized_cm, mean_df, var_df, overall_rms)\n",
    "\n",
    "        cov_inv_to_report(label, normalized_cm, letter)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(all_reports('_revised_report.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "This project contains six different trainable classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing finished, generate normalized central moments\n",
      "          M00          M02         M11          M20          M03          M12  \\\n",
      "0  131.088215  3797.278877  231.822154  2975.122165  1042.556578  2293.318746   \n",
      "\n",
      "          M21          M30  \n",
      "0  930.215497  2147.195582  \n",
      "Indexing finished, generate normalized central moments\n",
      "Indexing finished, generate normalized central moments\n",
      "Indexing finished, generate normalized central moments\n",
      "Indexing finished, generate normalized central moments\n"
     ]
    }
   ],
   "source": [
    "# all the means\n",
    "list_datasets = ['A', 'B', 'C', 'D']\n",
    "letter = list_datasets[0]\n",
    "label = 'data_set' + letter\n",
    "dataset = label +'.dat'\n",
    "datasetA = np.loadtxt(dataset)\n",
    "normalized_cm_A, mean_df_A, var_df_A, overall_rms_A = indexing_nms(datasetA)\n",
    "print(overall_rms_A)\n",
    "overall_rms_A = np.array(overall_rms_A)[0]\n",
    "\n",
    "def modified_all_rms(p, q, data, overall_rms_A):\n",
    "    moments = [] # Moments of 100 samples\n",
    "    overall_rms = overall_rms_A\n",
    "    M = []\n",
    "    mean = []\n",
    "    var = []\n",
    "    counter = 0\n",
    "    for index_p, index_q in zip(p,q):\n",
    "        column_names = \"M\" + str(index_p) + str(index_q)\n",
    "        M.append(column_names)\n",
    "        \n",
    "        central_moments = []\n",
    "\n",
    "        init = 0\n",
    "        last = 25\n",
    "        \n",
    "    \n",
    "        while last <= len(data):\n",
    "            \n",
    "            pattern, imean, jmean = centroid(init,last, data)\n",
    "            Mpq = mpq(pattern, imean, jmean, index_p, index_q)\n",
    "            central_moments.append(Mpq)\n",
    "    \n",
    "            # Put mean and var of the normalized central moments here\n",
    "            \n",
    "            # Direct answer for the variance in the set of means.\n",
    "            #np.var already divides by N rather than N-1\n",
    "            \n",
    "            # Break point. It's time to stop.\n",
    "            init += 25\n",
    "            last += 25\n",
    "            \n",
    "        # 100 central_moments will be divided into 10\n",
    "        RMS = rms(central_moments)\n",
    "        \n",
    "        moments.append(central_moments/overall_rms_A[counter])\n",
    "        counter += 1\n",
    "       \n",
    "        # Clearing\n",
    "        central_moments = []\n",
    "        \n",
    "    return M, pd.DataFrame(moments), overall_rms\n",
    "\n",
    "def modified_indexing_nms(data,overall_rms_A):\n",
    "    Mpq, moments, overall_rms = modified_all_rms(p,q,data,overall_rms_A)\n",
    "    normalized_cm = moments.transpose()\n",
    "\n",
    "    mean_df, var_df = mean_and_var(Mpq, normalized_cm)\n",
    "    mean_df.columns = Mpq\n",
    "    var_df.columns = Mpq\n",
    "    characters, indexes = index_title()\n",
    "\n",
    "    title_add = []\n",
    "    for i in range(len(mean_df)):\n",
    "        normalized_cm = pd.concat([normalized_cm, mean_df.loc[[i]]])\n",
    "        normalized_cm = pd.concat([normalized_cm, var_df.loc[[i]]])\n",
    "        title_add.append('mean('+ characters[i]+')')\n",
    "        title_add.append('var('+ characters[i]+')')\n",
    "\n",
    "    overall_rms = pd.DataFrame(overall_rms).T\n",
    "    overall_rms.columns = Mpq\n",
    "    normalized_cm = pd.concat([normalized_cm, overall_rms.loc[[0]]])\n",
    "\n",
    "    new_indexes = indexes + title_add + ['overall_rms']\n",
    "    \n",
    "    normalized_cm.index = new_indexes\n",
    "    print('Indexing finished, generate normalized central moments')\n",
    "    return normalized_cm, mean_df, var_df, overall_rms\n",
    "\n",
    "\n",
    "\n",
    "def taken(list_datasets):\n",
    "    for i in range(len(list_datasets)):\n",
    "        letter = list_datasets[i]\n",
    "        label = 'data_set' + letter\n",
    "        dataset = label +'.dat'\n",
    "        data = np.loadtxt(dataset)\n",
    "        normalized_cm, mean_df, var_df, overall_rms = modified_indexing_nms(data,overall_rms_A)\n",
    "        #print(mean_df)\n",
    "        #print(normalized_cm[0:100]) # No 0 to 100 to keep all the moments.\n",
    "    return mean_df, normalized_cm[0:100] # Result are mean_df, normalized_cm until the 100th line only\n",
    "\n",
    "mean_df_A, normalized_cm_A = taken(list_datasets[0])\n",
    "mean_df_B, normalized_cm_B = taken(list_datasets[1])\n",
    "mean_df_C, normalized_cm_C = taken(list_datasets[2])\n",
    "mean_df_D, normalized_cm_D = taken(list_datasets[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00161559  0.3571374   0.24560798  0.86172906 -0.64366066  0.5697219\n",
      "  -0.25145362 -1.17986173]]\n",
      "[[ 0.90778565  0.30668578  0.36234673  0.76457562 -0.6987141   0.44748917\n",
      "  -0.33112414 -1.13956964]]\n"
     ]
    }
   ],
   "source": [
    "#np.reshape(mean_df.iloc[0], (1,8))\n",
    "mu = np.array(mean_df_A[0:1])\n",
    "x = np.array(normalized_cm_A[0:1])\n",
    "print(mu)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclid_dist(x, mean):\n",
    "    distance = np.dot((x - mean), (x-mean).T)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06035872]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclid_dist(x, mu) # Testing for the a1 in compared to the mean a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "from numpy.linalg import inv\n",
    "def discriminant_function(x, mean_df, discriminant, label, letter, normalized_cm):\n",
    "    count = 0\n",
    "    minimum_dist = 0\n",
    "    container = [] # Container contains all the x_i - mu\n",
    "    average_cov, average_cov_inversed, container_cov, container_inv = cov_inv_to_report(label, normalized_cm, letter) \n",
    "    \n",
    "    for i in range(1,len(mean_df)+1):\n",
    "        k = i - 1\n",
    "        mu = np.array(mean_df[count:i])\n",
    "        mean = mu\n",
    "        \n",
    "        \n",
    "        #1 Minimum distance moment classifier\n",
    "        if discriminant == 1:\n",
    "            dist = float(euclid_dist(x, mu))\n",
    "            container.append(dist)\n",
    "           \n",
    "        #2 Bayes moment classifier with identical covariances.\n",
    "        if discriminant == 2:  \n",
    "            \n",
    "             \n",
    "            g_x_2 = abs(-0.5 * np.dot(np.dot((x - mean), average_cov_inversed), (x-mean).T)) #\n",
    "            container.append(g_x_2[0][0])\n",
    "        \n",
    "        #3 Bayes moment classifier with individual class covariances: quadratic decision function\n",
    "        if discriminant == 3:\n",
    "            \n",
    "            sigma_i = np.array(container_cov[k]) # i-th will follow the mean of each character.\n",
    "            sigma_i_inv = np.array(container_inv[k])\n",
    "            \n",
    "            g_x_3 = abs((-0.5 * np.log(np.linalg.det(sigma_i)) - (0.5 * (np.dot(np.dot(np.subtract(x, mean), sigma_i_inv), np.subtract(x,mean).T)) )))\n",
    "            container.append(g_x_3[0][0])\n",
    "            \n",
    "        #4 Bayes moment classifier using the first four moments\n",
    "        if discriminant == 4:\n",
    "            \n",
    "            x_1by4 = x[0][0:4]\n",
    "            mean_1by4 = mean[0][0:4]\n",
    "            \n",
    "            sigma_i = np.array(container_cov[k])\n",
    "            sigma_i_4by4 = sigma_i[0:4, 0:4]\n",
    "            sigma_i_inv_4by4 = inv(sigma_i_4by4)\n",
    "\n",
    "            g_x_4 = abs((-0.5 *  np.log(np.linalg.det(sigma_i_4by4))- (0.5 * (np.dot(np.dot((x_1by4 - mean_1by4), sigma_i_inv_4by4), (x_1by4-mean_1by4).T)) )) )\n",
    "            container.append(g_x_4)\n",
    "        count += 1\n",
    "    #saved = pd.DataFrame(np.dot(container, 100))\n",
    "    return container, saved\n",
    "  \n",
    "def figuring_letter(letters, container):\n",
    "    y = list(zip(letters, container))\n",
    "    for i in y:\n",
    "        if min(container) in i:\n",
    "            return i[0], i[1]\n",
    "\n",
    "def prediction(normalized_cm, mean_df, discriminant, label, letter, namefile):\n",
    "    count = 0\n",
    "    comparison = []\n",
    "\n",
    "    for i in range(1,len(normalized_cm)+1):\n",
    "        x = np.array(normalized_cm[count:i])\n",
    "        count += 1\n",
    "        \n",
    "        container, saved = discriminant_function(x, mean_df, discriminant, label, letter, normalized_cm)\n",
    "        char, score = figuring_letter(letters, container)\n",
    "        comparison.append(char)\n",
    "        #saved.T.to_csv(namefile, mode='a', float_format='%.f', index=None, header=None)\n",
    "\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using minimum distance moment classifier in Dataset A\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 's', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 9), ('r', 10), ('s', 11), ('x', 10), ('z', 10)]\n",
      "Prediction using minimum distance moment classifier in Dataset B\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 's', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'z', 'z', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'x', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 9), ('r', 10), ('s', 11), ('x', 9), ('z', 11)]\n",
      "Prediction using minimum distance moment classifier in Dataset C\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using minimum distance moment classifier in Dataset D\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 's', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 's', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 9), ('n', 11), ('o', 9), ('r', 10), ('s', 12), ('x', 10), ('z', 9)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "list_datasets = ['A', 'B', 'C','D']\n",
    "for letter in list_datasets:\n",
    "    normalized_label = \"normalized_cm_\" + letter\n",
    "    mean_label = \"mean_df_\" + letter\n",
    "    label = 'data_set' + letter\n",
    "    discriminant = 1\n",
    "    pred = prediction(eval(normalized_label), eval(mean_label), discriminant, label, letter, '1_'+letter+'.csv')# eval(mean_label)\n",
    "    \n",
    "    print(\"Prediction using minimum distance moment classifier in Dataset\", letter)\n",
    "    print(pred)\n",
    "    print(sorted(Counter(pred).items())) # We do count the number of the elements then sort it by the items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using Bayes moment classifier with identical covariances in Dataset A\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Bayes moment classifier with identical covariances in Dataset B\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Bayes moment classifier with identical covariances in Dataset C\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Bayes moment classifier with identical covariances in Dataset D\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "list_datasets = ['A', 'B', 'C','D']\n",
    "for letter in list_datasets:\n",
    "    normalized_label = \"normalized_cm_\" + letter\n",
    "    mean_label = \"mean_df_\" + letter\n",
    "    label = 'data_set' + letter\n",
    "    discriminant = 2\n",
    "    pred = prediction(eval(normalized_label), eval(mean_label), discriminant, label, letter, '2_'+letter+'.csv')\n",
    "    print(\"Prediction using Bayes moment classifier with identical covariances in Dataset\", letter)\n",
    "    print(pred)\n",
    "    print(sorted(Counter(pred).items())) # We do count the number of the elements then sort it by the items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using Bayes moment classifier with individual class covariances in Dataset A\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Bayes moment classifier with individual class covariances in Dataset B\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'n', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 11), ('o', 9), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Bayes moment classifier with individual class covariances in Dataset C\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Bayes moment classifier with individual class covariances in Dataset D\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "list_datasets = ['A', 'B', 'C','D']\n",
    "for letter in list_datasets:\n",
    "    normalized_label = \"normalized_cm_\" + letter\n",
    "    mean_label = \"mean_df_\" + letter\n",
    "    label = 'data_set' + letter\n",
    "    discriminant = 3\n",
    "    pred = prediction(eval(normalized_label), eval(mean_label), discriminant, label, letter, '3_'+letter+'.csv')\n",
    "    print(\"Prediction using Bayes moment classifier with individual class covariances in Dataset\", letter)\n",
    "    print(pred)\n",
    "    print(sorted(Counter(pred).items())) # We do count the number of the elements then sort it by the items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using Byes moment classifier with individual class covariances only first four moments in Dataset A\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'a', 'a', 'a', 'a', 's', 's', 'a', 's', 'a', 's', 'z', 'x', 'z', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 16), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 4), ('x', 8), ('z', 12)]\n",
      "Prediction using Byes moment classifier with individual class covariances only first four moments in Dataset B\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'c', 'c', 'o', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'x', 'n', 'e', 'n', 'x', 'n', 'n', 'n', 'n', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 2), ('e', 10), ('m', 10), ('n', 16), ('o', 10), ('r', 10), ('s', 10), ('x', 12), ('z', 10)]\n",
      "Prediction using Byes moment classifier with individual class covariances only first four moments in Dataset C\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'e', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 9), ('c', 10), ('e', 11), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Byes moment classifier with individual class covariances only first four moments in Dataset D\n",
      "['a', 'e', 'a', 'e', 'a', 'a', 'a', 'a', 'a', 'a', 'e', 'e', 'z', 'e', 'z', 'z', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'c', 'e', 'c', 'c', 'c', 'c', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'o', 'n', 'n', 'n', 'n', 'n', 'o', 'n', 'n', 'n', 'n', 'o', 'n', 'o', 'n', 'o', 'n', 'o', 'n', 'n', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'c', 'e', 'z', 'e', 'c', 'c', 'e', 'c', 'e', 'e']\n",
      "[('a', 8), ('c', 9), ('e', 19), ('m', 10), ('n', 14), ('o', 6), ('r', 10), ('s', 10), ('x', 10), ('z', 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "list_datasets = ['A', 'B', 'C','D']\n",
    "for letter in list_datasets:\n",
    "    normalized_label = \"normalized_cm_\" + letter\n",
    "    mean_label = \"mean_df_\" + letter\n",
    "    label = 'data_set' + letter\n",
    "    discriminant = 4\n",
    "    pred = prediction(eval(normalized_label), eval(mean_label), discriminant, label, letter, '4_'+letter+'.csv')\n",
    "    print(\"Prediction using Byes moment classifier with individual class covariances only first four moments in Dataset\", letter)\n",
    "    print(pred)\n",
    "    print(sorted(Counter(pred).items())) # We do count the number of the elements then sort it by the items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using Minimum distance classifier in binary pixel space in dataset A\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Minimum distance classifier in binary pixel space in dataset B\n",
      "['a', 'a', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'c', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'z', 'r', 'z', 'z', 'r', 'z', 'z', 'z', 'z', 'z', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 12), ('c', 11), ('m', 10), ('n', 18), ('o', 19), ('r', 2), ('x', 10), ('z', 18)]\n",
      "Prediction using Minimum distance classifier in binary pixel space in dataset C\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Minimum distance classifier in binary pixel space in dataset D\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'c', 'c', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 12), ('e', 8), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "\n",
    "def pred_dist_binary_pixel(data, data_target, init_target, last_target, function):\n",
    "    init = list(range(0, 2500, 250))\n",
    "    last = list(range(250, 2750, 250))\n",
    "    distances = []\n",
    "    for i, l in list(zip(init, last)):\n",
    "        a = list(range(i, l,25))\n",
    "        b = list(range(i+25, l+25, 25))\n",
    "\n",
    "        summa_pattern = 0\n",
    "        for n, m in list(zip(a,b)):\n",
    "            summa_pattern += data[n:m]\n",
    "\n",
    "        summa_pattern = np.flipud(summa_pattern)\n",
    "\n",
    "        Pi = (summa_pattern+1)/12 # (m+1)/(n+2) ==> Pi\n",
    "        #plt.imshow(Pi, cmap='gray', interpolation='nearest')\n",
    "\n",
    "\n",
    "        x_target= data_target[init_target:last_target]\n",
    "        x_target= np.flipud(x_target)\n",
    "        \n",
    "        if function == 1:\n",
    "            distance = np.sqrt(np.sum((x_target - Pi)**2))\n",
    "            distances.append(distance)\n",
    "        if function == 2:\n",
    "            g_x_6 = np.sum(abs(x_target * np.log(Pi) + (1-x_target) * np.log(1- Pi)))\n",
    "            distances.append(g_x_6)\n",
    "        '''\n",
    "        for n, m in list(zip(a,b)):\n",
    "            x_target= data[n:m]\n",
    "            x_target= np.flipud(x_target)\n",
    "\n",
    "            #5 Euclidian distance - Minimum distance classifier in binary pixel.\n",
    "            distance = np.sqrt(np.sum((x_target - Pi)**2))\n",
    "            distances.append(distance)\n",
    "\n",
    "        '''\n",
    "    char, score = figuring_letter(letters, distances)\n",
    "    saved = pd.DataFrame(np.dot(distances, 100))\n",
    "    return char, saved\n",
    "\n",
    "def iter_pred_dist_binary_pixel(data, data_target, function, namefile):\n",
    "    container=[]\n",
    "    init = list(range(0, 2500, 25))\n",
    "\n",
    "    last = list(range(25, 2525, 25))\n",
    "\n",
    "    for i, l in list(zip(init, last)):\n",
    "        char, saved = pred_dist_binary_pixel(data, data_target, i, l, function)\n",
    "        container.append(char)\n",
    "        #saved.T.to_csv(namefile, mode='a', float_format='%.f', index=None, header=None)\n",
    "    \n",
    "    print(container)\n",
    "    print(sorted(Counter(container).items()))\n",
    "    \n",
    "\n",
    "list_datasets = ['A', 'B', 'C','D']\n",
    "data = np.loadtxt('data_setA.dat')\n",
    "for i in list_datasets:\n",
    "    letter = i\n",
    "    label = 'data_set' + letter\n",
    "    dataset = label +'.dat'\n",
    "    data_target = np.loadtxt(dataset)\n",
    "    print('Prediction using Minimum distance classifier in binary pixel space in dataset', letter)\n",
    "    iter_pred_dist_binary_pixel(data, data_target, 1, '5_'+letter+'.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using Bayes classifier in binary pixel space in dataset A\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Bayes classifier in binary pixel space in dataset B\n",
      "['a', 'a', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'c', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'z', 'r', 'z', 'z', 'r', 'z', 'z', 'z', 'z', 'z', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 12), ('c', 11), ('m', 10), ('n', 18), ('o', 19), ('r', 2), ('x', 10), ('z', 18)]\n",
      "Prediction using Bayes classifier in binary pixel space in dataset C\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
      "Prediction using Bayes classifier in binary pixel space in dataset D\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'c', 'c', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "[('a', 10), ('c', 12), ('e', 8), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n"
     ]
    }
   ],
   "source": [
    "list_datasets = ['A', 'B', 'C','D']\n",
    "data = np.loadtxt('data_setA.dat')\n",
    "for i in list_datasets:\n",
    "    letter = i\n",
    "    label = 'data_set' + letter\n",
    "    dataset = label +'.dat'\n",
    "    data_target = np.loadtxt(dataset)\n",
    "    print('Prediction using Bayes classifier in binary pixel space in dataset', letter)\n",
    "    iter_pred_dist_binary_pixel(data, data_target, 2, '6_'+letter+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "    a   c   e   m   n  o   r   s   x   z\n",
      "a  10   0   0   0   0  0   0   0   0   0\n",
      "c   0  10   0   0   0  0   0   0   0   0\n",
      "e   0   0  10   0   0  0   0   0   0   0\n",
      "m   0   0   0  10   0  0   0   0   0   0\n",
      "n   0   0   0   0  10  0   0   0   0   0\n",
      "o   0   0   0   0   0  9   0   1   0   0\n",
      "r   0   0   0   0   0  0  10   0   0   0\n",
      "s   0   0   0   0   0  0   0  10   0   0\n",
      "x   0   0   0   0   0  0   0   0  10   0\n",
      "z   0   0   0   0   0  0   0   0   0  10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#Prediction using minimum distance moment classifier in Dataset A\n",
    "predicted_A = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 's', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "results = confusion_matrix(expected, predicted_A)\n",
    "print(results)\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "test = pd.DataFrame(results, index=letters, columns=letters)\n",
    "test.to_csv('confusion_1_A.csv')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  8  2]\n",
      " [ 0  0  0  0  0  0  0  0  1  9]]\n",
      "    a   c   e   m   n  o   r   s  x  z\n",
      "a  10   0   0   0   0  0   0   0  0  0\n",
      "c   0  10   0   0   0  0   0   0  0  0\n",
      "e   0   0  10   0   0  0   0   0  0  0\n",
      "m   0   0   0  10   0  0   0   0  0  0\n",
      "n   0   0   0   0  10  0   0   0  0  0\n",
      "o   0   0   0   0   0  9   0   1  0  0\n",
      "r   0   0   0   0   0  0  10   0  0  0\n",
      "s   0   0   0   0   0  0   0  10  0  0\n",
      "x   0   0   0   0   0  0   0   0  8  2\n",
      "z   0   0   0   0   0  0   0   0  1  9\n"
     ]
    }
   ],
   "source": [
    "#Prediction using minimum distance moment classifier in Dataset B\n",
    "predicted_B = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 's', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'z', 'z', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'x', 'z']\n",
    "\n",
    "results = confusion_matrix(expected, predicted_B)\n",
    "print(results)\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "test = pd.DataFrame(results, index=letters, columns=letters)\n",
    "test.to_csv('confusion_1_B.csv')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "    a   c   e   m   n   o   r   s   x   z\n",
      "a  10   0   0   0   0   0   0   0   0   0\n",
      "c   0  10   0   0   0   0   0   0   0   0\n",
      "e   0   0  10   0   0   0   0   0   0   0\n",
      "m   0   0   0  10   0   0   0   0   0   0\n",
      "n   0   0   0   0  10   0   0   0   0   0\n",
      "o   0   0   0   0   0  10   0   0   0   0\n",
      "r   0   0   0   0   0   0  10   0   0   0\n",
      "s   0   0   0   0   0   0   0  10   0   0\n",
      "x   0   0   0   0   0   0   0   0  10   0\n",
      "z   0   0   0   0   0   0   0   0   0  10\n"
     ]
    }
   ],
   "source": [
    "#Prediction using minimum distance moment classifier in Dataset C\n",
    "predicted_C = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "results = confusion_matrix(expected, predicted_C)\n",
    "print(results)\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "test = pd.DataFrame(results, index=letters, columns=letters)\n",
    "test.to_csv('confusion_1_C.csv')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  2  0  8]]\n",
      "    a   c   e   m   n   o   r   s   x  z\n",
      "a  10   0   0   0   0   0   0   0   0  0\n",
      "c   0  10   0   0   0   0   0   0   0  0\n",
      "e   0   0  10   0   0   0   0   0   0  0\n",
      "m   0   0   0  10   0   0   0   0   0  0\n",
      "n   0   0   0   0  10   0   0   0   0  0\n",
      "o   0   0   0   0   0  10   0   0   0  0\n",
      "r   0   0   0   0   0   0  10   0   0  0\n",
      "s   0   0   0   0   0   0   0  10   0  0\n",
      "x   0   0   0   0   0   0   0   0  10  0\n",
      "z   0   0   0   0   0   0   0   2   0  8\n"
     ]
    }
   ],
   "source": [
    "#Prediction using minimum distance moment classifier in Dataset D\n",
    "predicted_D = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 's', 's', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "results = confusion_matrix(expected, predicted_D)\n",
    "print(results)\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "test = pd.DataFrame(results, index=letters, columns=letters)\n",
    "test.to_csv('confusion_1_D.csv')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  9  1  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  1  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  9]]\n",
      "    a   c   e  m   n  o   r   s   x  z\n",
      "a  10   0   0  0   0  0   0   0   0  0\n",
      "c   0  10   0  0   0  0   0   0   0  0\n",
      "e   0   0  10  0   0  0   0   0   0  0\n",
      "m   0   0   0  9   1  0   0   0   0  0\n",
      "n   0   0   0  0  10  0   0   0   0  0\n",
      "o   0   0   0  0   0  9   0   1   0  0\n",
      "r   0   0   0  0   0  0  10   0   0  0\n",
      "s   0   0   0  0   0  0   0  10   0  0\n",
      "x   0   0   0  0   0  0   0   0  10  0\n",
      "z   0   0   0  0   0  0   0   1   0  9\n"
     ]
    }
   ],
   "source": [
    "#Prediction using minimum distance moment classifier in Dataset A\n",
    "#predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 's', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 9), ('r', 10), ('s', 11), ('x', 10), ('z', 10)]\n",
    "#Prediction using minimum distance moment classifier in Dataset B\n",
    "#predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 's', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'z', 'z', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'x', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 9), ('r', 10), ('s', 11), ('x', 9), ('z', 11)]\n",
    "#Prediction using minimum distance moment classifier in Dataset C\n",
    "#predicted =['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using minimum distance moment classifier in Dataset D\n",
    "predicted =['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 's', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 's', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 9), ('n', 11), ('o', 9), ('r', 10), ('s', 12), ('x', 10), ('z', 9)]\n",
    "results = confusion_matrix(expected, predicted)\n",
    "print(results)\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "test = pd.DataFrame(results, index=letters, columns=letters)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10]]\n",
      "    a   c   e   m   n   o   r   s   x   z\n",
      "a  10   0   0   0   0   0   0   0   0   0\n",
      "c   0  10   0   0   0   0   0   0   0   0\n",
      "e   0   0  10   0   0   0   0   0   0   0\n",
      "m   0   0   0  10   0   0   0   0   0   0\n",
      "n   0   0   0   0  10   0   0   0   0   0\n",
      "o   0   0   0   0   0  10   0   0   0   0\n",
      "r   0   0   0   0   0   0  10   0   0   0\n",
      "s   0   0   0   0   0   0   0  10   0   0\n",
      "x   0   0   0   0   0   0   0   0  10   0\n",
      "z   0   0   0   0   0   0   0   0   0  10\n"
     ]
    }
   ],
   "source": [
    "#Prediction using Bayes moment classifier with identical covariances in Dataset A\n",
    "predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Bayes moment classifier with identical covariances in Dataset B\n",
    "predicted =['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Bayes moment classifier with identical covariances in Dataset C\n",
    "predicted =['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Bayes moment classifier with identical covariances in Dataset D\n",
    "predicted =['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "results = confusion_matrix(expected, predicted)\n",
    "print(results)\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "test = pd.DataFrame(results, index=letters, columns=letters)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   c   e   m   n   o   r   s   x   z\n",
      "a  10   0   0   0   0   0   0   0   0   0\n",
      "c   0  10   0   0   0   0   0   0   0   0\n",
      "e   0   0  10   0   0   0   0   0   0   0\n",
      "m   0   0   0  10   0   0   0   0   0   0\n",
      "n   0   0   0   0  10   0   0   0   0   0\n",
      "o   0   0   0   0   0  10   0   0   0   0\n",
      "r   0   0   0   0   0   0  10   0   0   0\n",
      "s   0   0   0   0   0   0   0  10   0   0\n",
      "x   0   0   0   0   0   0   0   0  10   0\n",
      "z   0   0   0   0   0   0   0   0   0  10\n"
     ]
    }
   ],
   "source": [
    "#Prediction using Bayes moment classifier with individual class covariances in Dataset A\n",
    "predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Bayes moment classifier with individual class covariances in Dataset B\n",
    "predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'n', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 11), ('o', 9), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Bayes moment classifier with individual class covariances in Dataset C\n",
    "predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Bayes moment classifier with individual class covariances in Dataset D\n",
    "predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "results = confusion_matrix(expected, predicted)\n",
    "\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "test = pd.DataFrame(results, index=letters, columns=letters)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  0  0  0  0  3]\n",
      " [ 0  5  5  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  2  0  0  0  0]\n",
      " [ 0  0  0  0  6  4  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  4  5  0  0  0  0  0  0  1]]\n",
      "   a  c  e   m  n  o   r   s   x  z\n",
      "a  8  0  2   0  0  0   0   0   0  0\n",
      "c  0  0  7   0  0  0   0   0   0  3\n",
      "e  0  5  5   0  0  0   0   0   0  0\n",
      "m  0  0  0  10  0  0   0   0   0  0\n",
      "n  0  0  0   0  8  2   0   0   0  0\n",
      "o  0  0  0   0  6  4   0   0   0  0\n",
      "r  0  0  0   0  0  0  10   0   0  0\n",
      "s  0  0  0   0  0  0   0  10   0  0\n",
      "x  0  0  0   0  0  0   0   0  10  0\n",
      "z  0  4  5   0  0  0   0   0   0  1\n"
     ]
    }
   ],
   "source": [
    "#Prediction using Byes moment classifier with individual class covariances only first four moments in Dataset A\n",
    "predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'a', 'a', 'a', 'a', 's', 's', 'a', 's', 'a', 's', 'z', 'x', 'z', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 16), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 4), ('x', 8), ('z', 12)]\n",
    "#Prediction using Byes moment classifier with individual class covariances only first four moments in Dataset B\n",
    "predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'c', 'c', 'o', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'x', 'n', 'e', 'n', 'x', 'n', 'n', 'n', 'n', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 2), ('e', 10), ('m', 10), ('n', 16), ('o', 10), ('r', 10), ('s', 10), ('x', 12), ('z', 10)]\n",
    "#Prediction using Byes moment classifier with individual class covariances only first four moments in Dataset C\n",
    "predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'e', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 9), ('c', 10), ('e', 11), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Byes moment classifier with individual class covariances only first four moments in Dataset D\n",
    "predicted = ['a', 'e', 'a', 'e', 'a', 'a', 'a', 'a', 'a', 'a', 'e', 'e', 'z', 'e', 'z', 'z', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'c', 'e', 'c', 'c', 'c', 'c', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'o', 'n', 'n', 'n', 'n', 'n', 'o', 'n', 'n', 'n', 'n', 'o', 'n', 'o', 'n', 'o', 'n', 'o', 'n', 'n', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'c', 'e', 'z', 'e', 'c', 'c', 'e', 'c', 'e', 'e']\n",
    "#[('a', 8), ('c', 9), ('e', 19), ('m', 10), ('n', 14), ('o', 6), ('r', 10), ('s', 10), ('x', 10), ('z', 4)]\n",
    "results = confusion_matrix(expected, predicted)\n",
    "print(results)\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "test = pd.DataFrame(results, index=letters, columns=letters)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   c  e   m   n   o   r   s   x   z\n",
      "a  10   0  0   0   0   0   0   0   0   0\n",
      "c   0  10  0   0   0   0   0   0   0   0\n",
      "e   0   2  8   0   0   0   0   0   0   0\n",
      "m   0   0  0  10   0   0   0   0   0   0\n",
      "n   0   0  0   0  10   0   0   0   0   0\n",
      "o   0   0  0   0   0  10   0   0   0   0\n",
      "r   0   0  0   0   0   0  10   0   0   0\n",
      "s   0   0  0   0   0   0   0  10   0   0\n",
      "x   0   0  0   0   0   0   0   0  10   0\n",
      "z   0   0  0   0   0   0   0   0   0  10\n"
     ]
    }
   ],
   "source": [
    "#Prediction using Minimum distance classifier in binary pixel space in dataset A\n",
    "predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Minimum distance classifier in binary pixel space in dataset B\n",
    "predicted =['a', 'a', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'c', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'z', 'r', 'z', 'z', 'r', 'z', 'z', 'z', 'z', 'z', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 12), ('c', 11), ('m', 10), ('n', 18), ('o', 19), ('r', 2), ('x', 10), ('z', 18)]\n",
    "#Prediction using Minimum distance classifier in binary pixel space in dataset C\n",
    "predicted =['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Minimum distance classifier in binary pixel space in dataset D\n",
    "predicted =['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'c', 'c', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 12), ('e', 8), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "results = confusion_matrix(expected, predicted)\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "test = pd.DataFrame(results, index=letters, columns=letters)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   c  e   m   n   o   r   s   x   z\n",
      "a  10   0  0   0   0   0   0   0   0   0\n",
      "c   0  10  0   0   0   0   0   0   0   0\n",
      "e   0   2  8   0   0   0   0   0   0   0\n",
      "m   0   0  0  10   0   0   0   0   0   0\n",
      "n   0   0  0   0  10   0   0   0   0   0\n",
      "o   0   0  0   0   0  10   0   0   0   0\n",
      "r   0   0  0   0   0   0  10   0   0   0\n",
      "s   0   0  0   0   0   0   0  10   0   0\n",
      "x   0   0  0   0   0   0   0   0  10   0\n",
      "z   0   0  0   0   0   0   0   0   0  10\n"
     ]
    }
   ],
   "source": [
    "#Prediction using Bayes classifier in binary pixel space in dataset A\n",
    "predicted = ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Bayes classifier in binary pixel space in dataset B\n",
    "predicted = ['a', 'a', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'c', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'z', 'r', 'z', 'z', 'r', 'z', 'z', 'z', 'z', 'z', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 12), ('c', 11), ('m', 10), ('n', 18), ('o', 19), ('r', 2), ('x', 10), ('z', 18)]\n",
    "#Prediction using Bayes classifier in binary pixel space in dataset C\n",
    "predicted =['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 10), ('e', 10), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "#Prediction using Bayes classifier in binary pixel space in dataset D\n",
    "predicted =['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'c', 'c', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'm', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
    "#[('a', 10), ('c', 12), ('e', 8), ('m', 10), ('n', 10), ('o', 10), ('r', 10), ('s', 10), ('x', 10), ('z', 10)]\n",
    "results = confusion_matrix(expected, predicted)\n",
    "letters = ['a', 'c', 'e', 'm', 'n', 'o', 'r', 's', 'x', 'z']\n",
    "test = pd.DataFrame(results, index=letters, columns=letters)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
